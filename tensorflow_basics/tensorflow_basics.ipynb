{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Tensorflow?\n",
    "[Tensorflow](https://www.tensorflow.org/) (TF) is a software library created by Google that you can use to define complex mathematical functions and automatically compute their derivatives. Somewhat obvious by its name, one of its strengths is that it can works well with tensors (similar to how MatLab is short for Matrix Laboratory). \n",
    "\n",
    "Defining functions in TF (and some other machine learning libraries such as [Theano](http://deeplearning.net/software/theano/)) is somewhat different from how programmers conventionally define functions. In a traditional program, you define functions and then use them on your data. E.g. $$ c=a*b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Strictly Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "def multiply(a, b): return a*b\n",
    "x = 4\n",
    "y = 5\n",
    "print (multiply(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in TF, you define a computational graph which you later evaluate using your data. E.g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "mulitply = a * b\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run([mulitply], {a:4.0, b:5.0}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems far more complicated, doesn't it? There are a few things to notice. \n",
    "\n",
    "1. Variables of type `tf.placeholder()` with corresponding datatypes `tf.float32` had to be defined for every input to our function (or operation).\n",
    "2. Something called a `tf.Session()` had to be defined and was used for all subsequent computations. Afterwards it was closed.\n",
    "3. Variables had to be initialized.\n",
    "4. The operation `mulitply` had to be fed in with corresponding inputs to `sess.run()`.\n",
    "5. The session returned our answer in an array of length 1.\n",
    "\n",
    "So what are the benefits? Well, TF has a large library of available mathematical functions to use and it can automatically compute the derivates. As functions get more complicated, this becomes a godsend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore some of these components in a little more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and Variables\n",
    "In TF, you aren't defining functions as your normally would. Instead, you are defining a computational graph which stores information on variables and the operations they will go through. The two main types of variables you want to keep track of are\n",
    "1. **`tf.placeholder()`**\n",
    "    1. These are used to define inputs fors your computational graph which you can use to supply your own data. TF is similar to languages like C++ in that (at least input) variable's data types must be defined when they declared (e.g. `tf.float32`). [Here is a list of TF data types](https://www.tensorflow.org/resources/dims_types).\n",
    "2. **`tf.Variable()`**\n",
    "    1. These are in-memory bffers that contain tensors which hold and maintain parameters for your model. They must be explicitly initialized and can be saved to disk during or after training. For example, if you're training a neural network, a `tf.Variable()` will likely encapsulate the matrices which define the forward progogation across a layer of your network.\n",
    "\n",
    "There are a few useful things to knows that I will go over below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables have to be initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "m_p = tf.Variable(tf.ones((2,2)), name = \"variable_matrix\") # note that variables are defined similarly to numpy\n",
    "m = tf.zeros((2,2))\n",
    "sess = tf.Session()\n",
    "print (sess.run(m))\n",
    "sess.run(tf.global_variables_initializer()) # if you comment this out, this will not run with \n",
    "                                            # FailedPreconditionError: Attempting to use uninitialized value\n",
    "print (sess.run(m_p))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF provides global access to variables and facilitates the process with variable scopes\n",
    "Your model may end up having many variables that it depends on. For a neural network, you could for example have a matrix and a bias vector for each layer. If you have 7 layers, that's 14 variables. TF makes it easy to easily access these variables with a combination of variable scopes and global access via `tf.variable_scope()` and `tf.get_variable()`, respectively. Below I will define a simple 3 layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network(X):\n",
    "    layer1 = tf.Variable(tf.random_normal([3,3], stddev=0.1), name=\"matrix1\")\n",
    "    bias1 = tf.Variable(tf.zeros([1]), name=\"bias1\")\n",
    "    output1 = tf.matmul(X, matrix1) + bias1\n",
    "\n",
    "    layer2 = tf.Variable(tf.random_normal([3,3], stddev=0.1), name=\"matrix1\")\n",
    "    bias2 = tf.Variable(tf.zeros([1]), name=\"bias2\")\n",
    "\n",
    "    layer3 = tf.Variable(tf.random_normal([3,3], stddev=0.1), name=\"matrix1\")\n",
    "    bias3 = tf.Variable(tf.zeros([1]), name=\"bias2\")\n",
    "\n",
    "    return tf.matmul(m1, matrix2)\n",
    "\n",
    "## new\n",
    "def forward_pass(input, matrix_shape, bias_shape):\n",
    "    matrix = tf.get_variable(tf.random_normal(matrix_shape, stddev=0.1), name=\"matrix\")\n",
    "    bias = tf.get_variable(tf.zeros(bias_shape), name=\"bias\")\n",
    "    return tf.matmul(input, matrix1) + bias1\n",
    "\n",
    "def function(X):\n",
    "    with tf.variable_scope(\"matrix1\"):\n",
    "        output = matrix_multiplication(X, [3, 3], [3])\n",
    "    with tf.variable_scope(\"matrix2\"):\n",
    "        return matrix_multiplication(output, [3, 3], [3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now your model may have many models to track. To make things easier, TF provides `tf.variable_scope()` to make this easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Links\n",
    "\n",
    "1. [Stanford CS224 Introduction to Tensorflow](https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf)\n",
    "1. [Tensorflow Sessions](https://www.tensorflow.org/api_docs/python/client/session_management#Session)\n",
    "1. [Tensorflow Datatypes](https://www.tensorflow.org/resources/dims_types)\n",
    "1. [Tensorflow Variables](https://www.tensorflow.org/api_docs/python/state_ops/variables#Variable)\n",
    "1. [Tensorflow Variable Scopes](https://www.tensorflow.org/versions/master/how_tos/variable_scope/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deleted\n",
    "1. **Variables of type (`tf.placeholder()`) with corresponding datatypes (`tf.float32`) had to be defined for every input to our function (or operation). **\n",
    "  1. In TF, you are defining a graph which contains all the operations that will be performed on data and their order. `tf.placeholder()` is used to define inputs into the computational graph which you will later use as entry points for your data. TF is similar to languages like C++ in that a variable's data type must be defined when it is declated (e.g. `tf.float32`). [Here is a list of TF data types](https://www.tensorflow.org/resources/dims_types).\n",
    "\n",
    "2. ** Something called a `tf.Session()` had to be defined and was used for all subsequent computations. Afterwards it was closed.**\n",
    "  1. TF uses sessions to encapsulate the environment in which operations are executed and variable values are stored. By default, you only have one session running. However, you could run multiple sessions. Possibly utility? If you have a graph which defines operations by a neural network, you can simulate multiple learning instances of this neural network in different threads with their own corresponding sessions.\n",
    "\n",
    "3. **Variables had to be initialized.**\n",
    "\n",
    "4. **The operation (`c`) had to be fed in with corresponding data to `sess.run()`.**\n",
    "  1. TF will use `sess.run()` to run the defined computational graph to the point defined by the given operation. A dictionary variables and their values must be fed into `sess.run()`. The keys may either be the TF variables or their names. I will demonstrate this later.\n",
    "\n",
    "5. **The session returned our answer in an array of length 1.**\n",
    "  1. TF works with Tensors. Arrays are 1D tensors. Matrices are 2D tensors. Everything you get will be in the form of a tensor. As you start building graphs, tensor shapes will become very important. Be careful as TF tends to be finnicky.\n",
    "\n",
    "So what are the benefits? I see two primary ones. TF has a large library of available mathematical functions to use and it can automatically compute the derivates. As functions get more complicated, this becomes more of a godsend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
