{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to generate a sequence of one_hot vectors (i.e. a sequence of \"words\")\n",
    "def gen_sequence(words, embedding_size):\n",
    "    seq = np.zeros((len(words), embedding_size))\n",
    "    for i in range(len(words)):\n",
    "        seq[i,words[i]] = 1\n",
    "    return words\n",
    "\n",
    "# generate n_sentences sentences with variable number of words\n",
    "# Assume a vocabulary of size vocab_size\n",
    "# maximum words per sentence will be max_words_per_sent\n",
    "def generate_paragraph(vocab_size, n_sentences, max_words_per_sent):\n",
    "    sentences = []\n",
    "    for i in range(n_sentences):\n",
    "        nwords = np.random.randint(1, max_words_per_sent+1)\n",
    "        words = np.random.randint(vocab_size, size=nwords)\n",
    "        sentence = gen_sequence(words, vocab_size)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "        \n",
    "def print_sent_sizes(paragraph):\n",
    "    print ([len(s) for s in paragraph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 26]\n",
      "[14, 14, 6]\n",
      "[3, 3, 10, 1]\n",
      "[1, 3, 1, 2, 7]\n",
      "[2, 2, 3, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# generate 5 paragraphs\n",
    "vocab_size=30\n",
    "p1 = generate_paragraph(vocab_size, n_sentences=1+1, max_words_per_sent=int(1/1*vocab_size))\n",
    "p2 = generate_paragraph(vocab_size, n_sentences=2+1, max_words_per_sent=int(1/2*vocab_size))\n",
    "p3 = generate_paragraph(vocab_size, n_sentences=3+1, max_words_per_sent=int(1/3*vocab_size))\n",
    "p4 = generate_paragraph(vocab_size, n_sentences=4+1, max_words_per_sent=int(1/4*vocab_size))\n",
    "p5 = generate_paragraph(vocab_size, n_sentences=5+1, max_words_per_sent=int(1/5*vocab_size))\n",
    "\n",
    "paragraphs = [p1, p2, p3, p4, p5]\n",
    "for p in paragraphs: print_sent_sizes(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point onwards, we will treat these paragraphs as a single \"batch\".\n",
    "The goal is to to read the sentences from each paragraph one-at-a-time in parallel.\n",
    "This is somewhat of a difficult task because there are both a varible number of sentences and a variable number of words, which makes actions like \"bucketting\" and \"padding\" difficult and non-intuitive.\n",
    "\n",
    "I am practicing this for the context that every paragraph corresponds to a different example and can thus be treated independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function goes through every value in an array and finds the maximum length\n",
    "# will be applied to \"paragraphs\",which contains lists, for the maximum paragraph length \n",
    "#      and each paragraph, which contains np.arrays, for the maximum sentence length\n",
    "def get_max_length(array, size_op):\n",
    "    max_len = 0\n",
    "    for value in array:\n",
    "        max_len = max(max_len, size_op(value))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum_paragraph_length = 6\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "maximum_paragraph_length = get_max_length(paragraphs, len)\n",
    "print (\"maximum_paragraph_length =\", maximum_paragraph_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum_sentence_length = 24\n"
     ]
    }
   ],
   "source": [
    "# will be used for np.arrays\n",
    "def np_length(arr): return arr.shape[0]\n",
    "# this is slightly recursive. for each paragraph, I check the local maximum sentence length and \n",
    "# I then compare that against a \"global\" maximum sentence inside the main get_max_length function\n",
    "def max_sent_in_par(paragraph): \n",
    "    return get_max_length(paragraph, np_length)\n",
    "\n",
    "# sanity check\n",
    "maximum_sentence_length = get_max_length(paragraphs, max_sent_in_par)\n",
    "print (\"maximum_sentence_length =\", maximum_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# our goal is now to make a tensor for batch_size paragraphs, \n",
    "                                # with each paragraph containing maximum_paragraph_length sentences, \n",
    "                                # and each sentence containing maximum_sentence_length words,\n",
    "                                # and each word using an embedding size of vocab_size \n",
    "                                # (change this for whether you're using one-hot or word embeddings here)\n",
    "# i.e. a tensor of size batch_size X maximum_paragraph_length X maximum_sentence_length X embedding_size\n",
    "\n",
    "# let's remember that in our current form\n",
    "batch_size=5\n",
    "paragraphs_tensor = np.zeros((batch_size, maximum_paragraph_length, maximum_sentence_length, vocab_size))\n",
    "\n",
    "# fill out the tensor\n",
    "for i, paragraph in enumerate(paragraphs):\n",
    "    for j, sentence in enumerate(paragraph):\n",
    "        for k, word in enumerate(sentence):\n",
    "#             print (i, j, k)\n",
    "            paragraphs_tensor[i,j,k] = word\n",
    "\n",
    "# santiy check\n",
    "print (paragraphs_tensor[4][0][0] == p5[0][0])\n",
    "# this wil serve as a single batch that will be read in by tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's define a computational graph that will read in every tensor batch and process them with an rnn\n",
    "x = tf.placeholder(None, None, None, voca_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
