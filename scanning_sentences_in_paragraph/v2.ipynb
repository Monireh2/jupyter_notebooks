{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial on processing batches of paragraphs of sentences in tensorflow. \n",
    "A key assumption is that every sentence can be read independently--both within and across paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to generate a sequence of one_hot vectors (i.e. a sequence of \"words\")\n",
    "def gen_sequence(words, embedding_size):\n",
    "    seq = np.zeros((len(words), embedding_size))\n",
    "    for i in range(len(words)):\n",
    "        seq[i,words[i]] = 1\n",
    "    return words\n",
    "\n",
    "# generate n_sentences sentences with variable number of words\n",
    "# Assume a vocabulary of size vocab_size\n",
    "# maximum words per sentence will be max_words_per_sent\n",
    "def generate_paragraph(vocab_size, n_sentences, max_words_per_sent):\n",
    "    sentences = []\n",
    "    for i in range(n_sentences):\n",
    "        nwords = np.random.randint(1, max_words_per_sent+1)\n",
    "        words = np.random.randint(vocab_size, size=nwords)\n",
    "        sentence = gen_sequence(words, vocab_size)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "        \n",
    "def print_sent_sizes(paragraph):\n",
    "    print ([len(s) for s in paragraph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 12]\n",
      "[15, 15, 6]\n",
      "[7, 8, 8, 4]\n",
      "[3, 1, 6, 7, 3]\n",
      "[2, 2, 3, 6, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "# generate 5 paragraphs\n",
    "vocab_size=30\n",
    "p1 = generate_paragraph(vocab_size, n_sentences=1+1, max_words_per_sent=int(1/1*vocab_size))\n",
    "p2 = generate_paragraph(vocab_size, n_sentences=2+1, max_words_per_sent=int(1/2*vocab_size))\n",
    "p3 = generate_paragraph(vocab_size, n_sentences=3+1, max_words_per_sent=int(1/3*vocab_size))\n",
    "p4 = generate_paragraph(vocab_size, n_sentences=4+1, max_words_per_sent=int(1/4*vocab_size))\n",
    "p5 = generate_paragraph(vocab_size, n_sentences=5+1, max_words_per_sent=int(1/5*vocab_size))\n",
    "\n",
    "paragraphs = [p1, p2, p3, p4, p5]\n",
    "for p in paragraphs: print_sent_sizes(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point onwards, we will treat these paragraphs as a single \"batch\".\n",
    "The goal is to to read the sentences from each paragraph one-at-a-time in parallel.\n",
    "This is somewhat of a difficult task because there are both a varible number of sentences and a variable number of words, which makes actions like \"bucketting\" and \"padding\" difficult and non-intuitive.\n",
    "\n",
    "I am practicing this for the context that every paragraph corresponds to a different example and can thus be treated independently. Further, every sentence can be processed independently from another sentence. Thus our goal is to have a tensor with 3 indices, where the first index corresponds to the batch size, the 2nd to all sentences concatonated together, and the 3rd to the word embedding size. I.e. a tensor of dimension $\\sum_i^N |P_i| \\times S_{max} \\times E$, where $|P_i|$ is the number of sentences in the ith paragraph, $N$ is the number of paragraphs, $S_{max}$ is the maximum sentence length across all sentences, and $E$ is the embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences = 20\n"
     ]
    }
   ],
   "source": [
    "# First let's find the total number of sentences. sanity check. should be 20\n",
    "n_sentences = reduce((lambda x, y: x + y), [len(p) for p in paragraphs])\n",
    "print (\"number of sentences =\", n_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum_sentence_length = 15\n"
     ]
    }
   ],
   "source": [
    "# Now let's find the maximum sentence length.\n",
    "\n",
    "# this function goes through every value in an array and finds the maximum length\n",
    "# will be applied to \"paragraphs\",which contains lists, for the maximum paragraph length \n",
    "#      and each paragraph, which contains np.arrays, for the maximum sentence length\n",
    "def get_max_length(array, size_op):\n",
    "    max_len = 0\n",
    "    for value in array:\n",
    "        max_len = max(max_len, size_op(value))\n",
    "    return max_len\n",
    "\n",
    "# will be used for 2D np.arrays \n",
    "def np_length(arr): return arr.shape[0]\n",
    "\n",
    "# this is slightly recursive. for each paragraph, I check the local maximum sentence length and \n",
    "# I then compare that against a \"global\" maximum sentence inside the main get_max_length function\n",
    "def max_sent_in_par(paragraph): \n",
    "    return get_max_length(paragraph, np_length)\n",
    "\n",
    "# sanity check\n",
    "maximum_sentence_length = get_max_length(paragraphs, max_sent_in_par)\n",
    "print (\"maximum_sentence_length =\", maximum_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now that we know the number of sentences and the paragraph length, we can create a 3D tensor containing all of the paragraphs\n",
    "\n",
    "paragraphs_tensor = np.zeros((n_sentences, maximum_sentence_length, vocab_size))\n",
    "# fill out the tensor\n",
    "s = 0\n",
    "for i, paragraph in enumerate(paragraphs):\n",
    "    p_length = len(paragraph)\n",
    "    for j, sentence in enumerate(paragraph):\n",
    "        for k, word in enumerate(sentence):\n",
    "            paragraphs_tensor[s,k] = word\n",
    "        s += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a computational graph that will read in every tensor batch and process each sentence with an rnn. I will use a basic RNN following [this tutorial on RNNs](https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/) and [this tutorial on variable length sequences](https://danijar.com/variable-sequence-lengths-in-tensorflow/) by [Danijar Hafner](https://danijar.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, None, vocab_size))\n",
    "try:\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units=256, state_is_tuple=True)\n",
    "except:\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units=256, state_is_tuple=True, reuse=True)\n",
    "output, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n"
     ]
    }
   ],
   "source": [
    "output = tf.transpose(output, [1, 0, 2])\n",
    "# print(output.get_shape()[0])\n",
    "# last = tf.gather(output, int(output.get_shape()[0]) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
